{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Alternus Vera.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxOYiQ0FzpOkUUjUuSXp9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RonakMehta21/Machine-Learning/blob/master/LSTM%20with%20pre-trained%20GloVe%20Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUdJdKnsRxXk",
        "colab_type": "code",
        "outputId": "e4ffe336-92fe-4bc3-ab54-ac34d4f93180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import gensim\n",
        "from sklearn.externals import joblib\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import nltk\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# System related libraries\n",
        "import os\n",
        "import re\n",
        "import fnmatch\n",
        "import importlib\n",
        "import multiprocessing\n",
        "from joblib import Parallel, delayed\n",
        "from datetime import datetime\n",
        "from time import time\n",
        "\n",
        "# Formatting\n",
        "import pprint\n",
        "from tabulate import tabulate\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "\n",
        "# Handle table-like data and matrices\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Collections\n",
        "import operator\n",
        "from itertools import product, compress\n",
        "from functools import reduce\n",
        "from operator import itemgetter\n",
        "from collections import defaultdict, Counter\n",
        "from glob import glob\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "# Data Visualization\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# String, NLTK, Gensim libraries\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.models import FastText\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, Normalizer, normalize, scale, FunctionTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.neighbors import KernelDensity, KNeighborsClassifier\n",
        "from skimage.transform import resize as imresize\n",
        "\n",
        "# Modelling Algorithms\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, make_scorer, confusion_matrix, classification_report, log_loss, auc, roc_auc_score, roc_curve, f1_score, precision_recall_curve, recall_score, precision_score\n",
        "\n",
        "# Keras\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, BatchNormalization\n",
        "\n",
        "from keras.models import Model, Sequential, model_from_json, load_model\n",
        "\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard, Callback, CSVLogger\n",
        "from keras import backend as K\n",
        "\n",
        "# Model Optimization\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, rand, pyll\n",
        "from hyperopt.pyll.base import scope\n",
        "from hyperopt.pyll.stochastic import sample\n",
        "\n",
        "from scipy.stats.kde import gaussian_kde\n",
        "\n",
        "# Visualization using Matplotlib, Seaborn, Plotly\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('white')\n",
        "\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "py.init_notebook_mode(connected=True)\n",
        "\n",
        "%matplotlib inline\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "# Code source: https://degravek.github.io/project-pages/project1/2017/04/28/New-Notebook/\n",
        "# Dataset from Chakraborty et al. (https://github.com/bhargaviparanjape/clickbait/tree/master/dataset)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1AMpIX60wKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read text file into colaboratory\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7CeIgdh00VK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjGbT_BeNgyY",
        "colab_type": "code",
        "outputId": "5cc7b430-cf70-476c-b5bd-86096f2149e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Google drive link where the dataset is stored.\n",
        "link = 'https://drive.google.com/open?id=19N9D6JONrA75Pd32hl228UoCxZs4K2cF'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19N9D6JONrA75Pd32hl228UoCxZs4K2cF\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIzo3bVoNlWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the text file.\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('fake_news_train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os2uOFyRNq6X",
        "colab_type": "code",
        "outputId": "dde2a9ed-f7d6-433c-935e-3f54d3a327e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Google drive link where the dataset is stored.\n",
        "link = 'https://drive.google.com/open?id=1QP8PL-QCAH2nJto7IBVI_k7TaG0K76G0'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1QP8PL-QCAH2nJto7IBVI_k7TaG0K76G0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLCgJE9OOL_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the text file.\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('fake_news_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs_RNVzjs7wc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1664cd4-c4f9-456d-f18a-c6810cf78d5a"
      },
      "source": [
        "# Google drive link where the dataset is stored.\n",
        "link = 'https://drive.google.com/open?id=1IY6TLlXcH27-HQhAOu7KoxvHl1nvymO0'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1IY6TLlXcH27-HQhAOu7KoxvHl1nvymO0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKtoqUdstOHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the text file.\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('glove.6B.300d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0StGIjdrOkSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('fake_news_train.csv')\n",
        "test = pd.read_csv('fake_news_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9BGlscsOl-X",
        "colab_type": "code",
        "outputId": "c1937413-3a26-40c6-bfba-b80c17a8589b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "# Display check the dimensions and the first 2 rows of the file.\n",
        "print('Fake News Dataset')\n",
        "print('train dim:',train.shape, 'test dim:', test.shape)\n",
        "train.iloc[0:2]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake News Dataset\n",
            "train dim: (20800, 5) test dim: (5200, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... label\n",
              "0   0  ...     1\n",
              "1   1  ...     0\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTabAedu2F3i",
        "colab_type": "text"
      },
      "source": [
        "## **Data Cleaning and Text Preprocessing**\n",
        "\n",
        "*Steps included in the preprocessing:*\n",
        "\n",
        "* Remove Special Characters and Punctuations\n",
        "* Lower case the news\n",
        "* Tokenization\n",
        "* Remove Stop Words\n",
        "* Lemmatization\n",
        "* Stemming\n",
        "* Spell Check\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PwjSsLO2d1w",
        "colab_type": "text"
      },
      "source": [
        "## Putting It All Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfO029cu2gGd",
        "colab_type": "text"
      },
      "source": [
        "To make the code reusable, we need to create a function that can be called many times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKsh4BCK2I_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def cleaning(raw_news):\n",
        "    import nltk\n",
        "    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", str(raw_news))\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    # 4. Convert the stopwords list to \"set\" data type.\n",
        "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    \n",
        "    # 5. Remove stop words. \n",
        "    words = [w for w in  news_words  if not w in stops]\n",
        "    \n",
        "    # 6. Lemmentize \n",
        "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "    \n",
        "    # 7. Stemming\n",
        "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "    \n",
        "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "    return \" \".join(stems)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SGxFM_WPKp-",
        "colab_type": "code",
        "outputId": "a10e4516-4dde-4f35-e352-c5ee72b4275c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import time\n",
        "# clean training and test data \n",
        "# create new column \"tokenized\"\n",
        "t1 = time.time()\n",
        "\n",
        "# Add the processed data to the original data. \n",
        "# Perhaps using apply function would be more elegant and concise than using for loop\n",
        "train['clean'] = train['text'].apply(cleaning) \n",
        "\n",
        "t2 = time.time()\n",
        "print(\"\\nTime to clean, tokenize and stem train data: \\n\", len(train), \"news:\", (t2-t1)/60, \"min\")\n",
        "\n",
        "t1 = time.time()\n",
        "test['clean'] = test['text'].apply(cleaning)\n",
        "\n",
        "t2 = time.time()\n",
        "print(\"\\n\\nTime to clean, tokenize and stem test data: \\n\", len(test), \"news:\", (t2-t1)/60, \"min\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Time to clean, tokenize and stem train data: \n",
            " 20800 news: 3.5365283648173014 min\n",
            "\n",
            "\n",
            "Time to clean, tokenize and stem test data: \n",
            " 5200 news: 0.8856431444485983 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uYZyKMc4EJr",
        "colab_type": "code",
        "outputId": "376fd44c-8566-4445-dd6b-0e12873b0b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df_fake = train[['clean','label']]\n",
        "df_fake"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hous dem aid even see comey letter jason chaff...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ever get feel life circl roundabout rather hea...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>truth might get fire octob tension intellig an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>video civilian kill singl u airstrik identifi ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>print iranian woman sentenc six year prison ir...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20795</th>\n",
              "      <td>rapper unload black celebr met donald trump el...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20796</th>\n",
              "      <td>green bay packer lost washington redskin week ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20797</th>\n",
              "      <td>maci today grew union sever great name america...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20798</th>\n",
              "      <td>nato russia hold parallel exercis balkan press...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20799</th>\n",
              "      <td>david swanson author activist journalist radio...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20800 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   clean  label\n",
              "0      hous dem aid even see comey letter jason chaff...      1\n",
              "1      ever get feel life circl roundabout rather hea...      0\n",
              "2      truth might get fire octob tension intellig an...      1\n",
              "3      video civilian kill singl u airstrik identifi ...      1\n",
              "4      print iranian woman sentenc six year prison ir...      1\n",
              "...                                                  ...    ...\n",
              "20795  rapper unload black celebr met donald trump el...      0\n",
              "20796  green bay packer lost washington redskin week ...      0\n",
              "20797  maci today grew union sever great name america...      0\n",
              "20798  nato russia hold parallel exercis balkan press...      1\n",
              "20799  david swanson author activist journalist radio...      1\n",
              "\n",
              "[20800 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxzv5XUXqAYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "9167c985-c29e-401e-d7a0-1ed88064f6e8"
      },
      "source": [
        "# Encode labels into categorical values\n",
        "\n",
        "train = df_fake.clean\n",
        "label = df_fake.label\n",
        "\n",
        "# Plot of label types numbers\n",
        "pd.Series(label).value_counts().plot(\n",
        "    kind='pie', title='Labels Distribution', figsize=(5, 5))\n",
        "plt.show()\n",
        "\n",
        "# Encode labels and create classes\n",
        "le = LabelEncoder()\n",
        "le.fit(label)\n",
        "label_encoded = le.transform(label)\n",
        "print(\"\\n\\nClasses: \", le.classes_)\n",
        "\n",
        "# Convert labels into categorical values\n",
        "label_onehot = to_categorical(label_encoded)\n",
        "print(\"\\nNumber of One Hot encoded class labels: \", label_onehot.shape[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS8AAAEvCAYAAAAU3kfYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3wb5f0H8M9Jsoa1vHfsOLbz2HF2IAUChYawW/YepVAC/dGSsmdboJQfKaPMFpJCaKEEUspqGYUwwgo/VnZsP/GIHe8Rb8vWvN8fJ4MxtuP93J2+79cr2JJOp4+E9dHd6dEjSZZlEEKI1hhEByCEkPGg8iKEaBKVFyFEk6i8CCGaROVFCNEkKi9CiCZReWkUY2wmY0xmjB2uhvWM4fY2McaenKJ138EYKxvu9BTc3t8YY+9O1frJyEyiA0QqxtjfAGRwzleIzjJRjLFNAI4Mn/QDaAOwC8CLAJ7inPsHLH46gMAo15sBoBrAjzjnm0ZxlfsBPDa61KPHGLsQwLOcc2nQRb8GbQAIQ+VFJst6ANcBMAJIBnA0gP8FcBFj7BjOuQcAOOetk33DjDEDAIlz3g2ge7LXPxzOecd03Rb5PiovlWKMnQ/llT0fytbM5wCu4ZzvGbToTMbYnQAOA1AP4FbO+QsD1pMM4I8ATgJgBbADwC2c849GuO1bAVwGIB1AJ4AtAE7lnPeOELmXc94Q/r0WwBbG2NsAvgZwA4A7w+veBKCMc35Z+PTh4Xzzw9etAHAj5/xtKFtdAPABYwwAqjjnMxljdwC4EMBt4fXmApjHGDsHwIWc89xB9+d8AH8AkAbgYwArOeeV4cvuGHydcKaPAWQDmAng2fD5/R9H+Tvn/GeDt54ZYxKUAr8SQP9W46Oc84cGrLsSwDMA3AAugvL/dj2AGzjno9oiJQra5FUvC5Qn3GIAxwAIAniDMWYetNy9ANYBWAjlSfAcY2wRADDGbAA+AOAEcAKARQDeBLCRMVYw1I0yxk4HcDOU4swL3/Zb47kDnPMdAP4L4KxhbssE4N9Qinlx+N8dADzhRRaHf54BIBXAwQOungalJC4GMAdAzTAxUsPLnQ3gCAAuAC+Hi2Y0NgP41YB1pUJ5bIZyJYC7AKwGUAjgPgCrGWM/H7TcVVBeaH4Q/v1X4ftBxoC2vFSKc/70wNOMsZ8B2A/lCfzpgIue4pw/F/79N4yx5QCuhfKqfg6UJ+s5A17V72aMHQ3gCgBXD3HTWQAaAPw3fKxqH4BtE7gru6HsQg7FCSAWwL8556Xh80oHXN4c/tk6YKuunxXARZzzff1nhLfOBosG8DPOeVl4mYsAcADLAbx3oPCccx9jrCP8++AMg90MZUtrbf99YUqo2wA8NWC5jznnqwcscwmAFYOWIQdA5aVSjLGFAG6HskWVAKB/SyEL3y2vzwZd9VN8WxYHA0gB0D7oiW0BMNwu4D8BrAJQxRh7B8oT/FXOedf47gkkAEN++p9z3hZ+5/Ftxtj7AD4E8ArnnI9ivY0Di2sEzf3FFb7NPYyxFihbRgcsr9FijLmg7CoO3h3/EMCvGWPR/cf98P0Xgzoou6hkDGi3UYUYY9EA3oHypL8EwFIoRSQDGLzbOBIDgGIoBTjwXwGAlUNdgXNeC+U426UAmgD8FgBnjM0Yz32BUhIVw13IOV8JYAmAjVDesdzFGLtiFOvtGWeewUL49oWhX9QkrXs4vkGnZdBzcczoAVOnAgCJAG7jnG/inBdD2b0a6jjNIYNOHwagKPz7VwBmAejknJcN+lc33I1zzr2c8/9yzm8EMA/KrtepY70TjLH5AI6DMmRiWJzzXZzzP3HOT4Cy63R5+KL+J7lxrLc9QCJjLGdAptlQtmT7H6MmAEmMsYG3sRjf5Qtfd9gcnPNOKMfdfjjooiMB7B2w1UUmCe02iuUI7x4O1AegCoAXwFWMsQegvOO1GkPvfv2cMVYCpaguBHAolIPAAPAcgGugHOi/DcAeKMMYlgMo5py/Onhl4YPLBgBfAGiHsgvqxLdP9uHYGGMpUIomCcoxnFvC67l/qCswxnKhbAH+B8o7c2lQDqpvCS/SAmXow7GMsd0AvJzztgPkGMwD4GnG2LXh049C2W3r32X8AEo5/54xtg5Kcf1y0Dr2hn+ezBj7BMo7q0MNybgHwAOMsVIAm6A8zv8zxPrIJKAtL7F+AGDroH+vcs5boBTRMVAOeN8P4HoouziD3QxlS2UHlIP0F3LOtwAA57wPyiv/VwCehlJeL0PZDa0aJlMblF3VTVB2Oa8FcDnn/EDHh86H8g5aJYC3ARwP4FYAR42w1dED5R3NF8LZXsKAd/c45yEoT/yzoWzVbD1AhqHUA1gL4F8APoFSZqdzzuXwbXAoBXoelIG1l4Zzf4Nz/iWAhwGsgbKlNtxA2McB/C58/SIANwG4mXNOB+KngEQzqRJCtIi2vAghmkTlRQjRJCovQogmUXkRQjSJyosQoklUXoQQTaLyIoRoEpUXIUSTqLwIIZpE5UUI0SQqL0KIJlF5EUI0icqLEKJJVF6EEE2i8iKEaBKVFyFEk6i8CCGaRHPYk2kTniP+xwCaOOdzRech2kZbXmQ6/Q3K3PaETBiVF5k2nPOPALSKzkH0gcqLEKJJVF6EEE2i8iKEaBKVFyFEk+hLZ8m0YYw9D+AoAAkAGgHcTt8mTcaLyosQokm020gI0SQqL0KIJlF5EUI0icqLEKJJVF6EEE2iWSXIdDMCiILytxcFQAYQBBAI/wsCCAlLRzSDyotMlAQgBkBqyNMzI9jcUCD7fXmhvt4E2dPjkPt6nSFvnxM+n0MOBk0IBSXZ75fkgM+AQEDZ8jeZQpLRJMNokiWTSZZMJj/Mlm6DxdotWW3dki2622CNbkNU1F5jTBw3xsRVAmgA0ASl8EgEovIiYxEd6utdFKjdd1Koq6Mw2N6aKvd0xQZbW6IDDbWOQHODM7i/WQq1tUD2eif3lk0mGGPiYYxLgDEhqdeYlNZpSkrpNTjdrcaYuHqDy11qSkl/0+BwfQmgfXJvnKgRDVIlw7HJPt8Cf03lSaHO9rnB9ta0YEtTsnf31mTfnt3WULu6ZraRHE6Yc/L9lrmLmkzJaU2GmPh6ozumwpiY+pbRHfM5gP2iM5LJReVF+hlCnp5D/dV7Lws21uUHmhtTvbu3JPn2FNlCbdp83ks2O8w5LGiZu6jRNCO70ZScyk0pGc8aY+PfBeATnY9MDJVXZLMFmupPDjbVnxuor8nr/fzjmX1bNtsnfZdPLUwmWOYu9kYvO7rKlDaj3JiY/GZUetbzoK0yTaLyijxJ/n0VFweaG1cEaipzPB9vzPLxXSaEIu8NPlPmLDn6h8fWmmfm7jUmpXxpzslfC4CLzkVGh8orMrj8VeVXBeqqj/eVFc/yfPROWqCuWnQmVTG4YmA77Eet1gVLK03pMz425+T/EUC96FxkeFRe+iUFO9oOC9RUXu8rK5nf/Z8N2YH6Gkl0KC0wxMTBceIZtZbCRUWm1Iy/mpLTXoYy/oyoCJWX/tj8+yqu8tfuO7P3443M8+l7LgRoKNR4WeYt8dqPO7U0asbMd825BXcDaBGdiSiovPQj3ct33+7fu+eIrlfX5wWq9xpFB9ITQ2w8nKecX2WZM39LVGbOXQana6voTJGOykvjQt1dBf7K0nv7tn+1sPvfL2SEujtFR9I3UxTsR5/UGn34il2mzOzfmRKSPxQdKVJReWlXmrd4x0O9n206vOu151MR8IvOE1kkCdE/OrHNfuzJX0dl5V5ndLl3iI4Uaai8tMft21N0b9/2L47r3PB0ltzbIzpPZDOZ4Djp7Kbow5dvNufNuVqKMleJjhQpqLy0w+qr2PNbH995Zsdza2drddS7XkkWK5xnXlxjXXLoBxY291rQgf0pR+Wlfkb/vopf+Sr2XNb53Jo5gbpqmoNNxQxON1znraywFC58w5ybfyuAbtGZ9IrKS8X8tft+HKipvKPzhafm+vbstojOQ0bPmJAM94W/4ObZc/4clZXzGJR5y8gkovJSJ4eX71rT/ca/jve893qc6DBk/CwLDu5xX3D5Z5bCRT8FjdifVFReKuOv3Xe8v7L0vrbH751Lx7X0QbLaEHPpr8stcxc9TFthk4fKSz2ivXt2P9Hz31dO7Hn71XjRYcjkG7AVdjGAOtF5tI7KSwUC9TUrfJWlf2p//N55wf3NouOQKSRZbXBfsqrcOm/xI1FZOY+CtsLGjcpLLJtvT9Gfuze+9uOeN19KFB2GTB/L/IN63BdeQVthE0DlJUiguXGJv5w/1fbEvfODzQ0020MEkixWxKy8psJSuPi2qMzsF0Tn0RoqLwH8VeWX9H7+0Z0dz/xlBujxj3j2405tdvz4rHXmWewW0G7kqFF5TS/JV1b8p65X1//U88FbNASCfMNcsMATs/LajRZWeB6AXtF5tIDKa/pEe0t2bmh74t4V/tJiq+gwRH2M8Uly3HV3fmUpmH+aZLbUis6jdlRe00D2eTO8u7e+uv+B25fQ2C0yEslsQexVt+2xFMy/3JSaQdPtjIDKa4oF6muO8hZtX9v66N158NO3bZHRcZ1zaa3tiGPuM2fnPSw6i1pReU0h397SVb0fb7ypc8O6NNFZiPbYDjmyw3XuZS+Z8wquAEBzeQ9C5TU1JF9p8cOdG576ae9nm9yiwxDtMmXO8sdd/buNFjb3NNAX5X4Hldfkk3ylxWvan/zTBd5dW6NFhyHaZ0xOC8Xf8If3LAXzTwbQJzqPWlB5TS7JV1r0dNsT953tK9lpEx2G6IcxIVmOv+nuDy1zFp4EwCM6jxpQeU0eg6+06NnWP99zOg2FIFPBEBuPhJtXf2KZu+gE0CSHVF6TRPKVFv2j9ZE/nOmv2GMWHYbol8Edi4Tb7v3EUrjoOET4FhhNKTxxkq+0+Km2P68+g4qLTLVQRxta7rn5cG/RtjcARPQWPpXXBPnKih9rW/vAOb7SIpqmmUyLUNt+7F9965He4h3/ARCxL5hUXhPgKyu5v/1vj/3UV7SN3lUk0yq4v0naf+9ty70lO18BEJHfjk7lNU6+yrJVnRvWXebd+rlDdBYSmYJN9YbWR/5wjK+0+C+is4hA5TUOgYbaI3o/effG3s3v0wBUIlSgqjyq85/rzvbtLf2V6CzTjcpr7FK9JTv/2vn8k+migxACAL2bP4jp/eS9mwMNtUeIzjKdqLzGxtq3/ctX2h65i4kOQshAnS88me4t2v6k7PNGzIsqldfoSV6+6x+tD965VPZ6RWch5HvaHr17trdo+ysAIuLTHVReo+Qr53d2rHv4BJpvnqiV7POi9U+3H+Tlu9YD0P3fKZXXKPhrKk/reefVK+iD1kTtgvubpfa1DxznKytZLTrLVKPyOoBQV+ds7/av7u9+/cUk0VkIGQ1fyU5b9+v/vNS/r+J80VmmEpXXyMzePbvXt625b5boIISMRc/Gfyd4d2/7PYAU0VmmCpXXCHxlxQ+2/+WPixAMio5CyJi1P/VQjnf3tmeg0+NfVF7DCDQ3HOL58J3TAw019BgRTZJ7PehYv/Ywf1X5L0VnmQr0xBya2V9V8eeuV/6h201uEhm8276we3dtvRpAqugsk43Kawi+suIH2tfct4C+zZroQfu6h3O8u7fqbveRymuQQEvjQZ6P3z0jUFcdkZ/UJ/oj9/Wi47m1h/qrynX1+Ucqr++K8ldVPNH18rO628Qmkc27/Ut7386vrwagm6/ho/IawFdWfF/7mvsXIhQSHYWQSdex7pFZetp9pPIKC7a2LPZ88t7Zgdoq2l0kuiR7+9Dx7BOH+vaWrhKdZTJQeSkkf1X5o10vPUO7i0TXvDu/jvaXl1wBwC46y0RReQHw11Sd2/XyP2h3kUSEjufW5PvKS+4SnWOiqLwAY6C26rq+LZ/Rh65JRAg2NUje3dtPARAvOstERHx5+fdVXNm5Yd1c0TkImU6dz6+d5eW7HhCdYyIivbzM/sryy3x8F31tGYkooc4O9G357GjZ58sUnWW8Irq8fHv33NKxfm2h6ByEiND14t8zfKVFD4nOMV6RXF4OX1nJOYHqvTQ0gkQk2dsHz6fvLwt2dswXnWU8Ira8fGXFd3U+tzZfdA5CROp+fUOSv7JUk8e+IrW84r1F239C89GTiBcMomfjf5YEmhuWi44yVhFZXl6+64+dz/81R3QOQtTA88GbsYHqyttF5xirSCwvh6+06Iehzg7ROQhRB1mGZ/P7hSFPzwLRUcYi4srLV1l6Tfer63NF5yBETXrefT3ev7f0d6JzjEWklZcUqN13aqC+ho51ETKQ3wdv8Y7FABJERxmtiCqvQFP9T3refo3eYSRkCF2vrZ/pKy+5VXSO0Yqs8qrdt4o+w0jI0EKtLfDv27sCgCbGPkZSeWX07fy6gOalJ2R4Pe+8lhdorDtNdI7RiJjy8pUW39Tz1su6mQKXkKng3fGVNVBfc7noHKMRKeVl8NdW/TDU2S46ByGq5929dQ408FVpEVFegeaGEz3vvZEnOgchWtD95kvpvtLim0TnOJDIKK+66l/0bf0/m+gchGhBqL0Vgfrqw0XnOJBIKC9zoKYqjw7UEzJ6fTu3zASg6r0V3ZdXsG3/0Z7N72eJzkGIlvRufj/eV85Xis4xEt2XV6C+5iLvri00UyohYxBqb0WwueEHonOMRO/lJQWa6vMRCIjOQYjm+CrLsgHEic4xHL2XV753+5czRYcgRIs8H76d4a+pOk90juHourx85SUre/9vU6zoHIRoUWBfhRRsbjhJdI7h6Lq8Ak0NB9G8XYSMX6CuOgeAWXSOoei5vOL9yj47IWScPJvfzwq2thwjOsdQdFte/tqq8zwfvZMuOgchWubd+bUlUF9zkegcQ9FteQWbG08I7KugSQcJmYhgEIGmOlUOVtVvebU0ZojOQIgeBOqqkwGo7o0vvZZXor+uWjPT2RKiZt5dW5KDXR2qG7Cqy/IKdrYv9e3amiw6ByF64CsrMQUb608QnWMwfZZXU/3xvgquialsCVE72dODYEeb6r5xS5/l1d42S+71iI5BiG6E2ltVNzmhLssr1NGaIjoDIXoSaKpLAhAjOsdAeiwvd6CpPkl0CEL0xLtzS1Kou2up6BwD6a68Qj3dS7x0sJ6QSeUrK44KNNSq6qC97sor0FBzgr+sJEp0DkL0RO7pRqizTVWDVXVXXqHOdhbq7hQdgxDdCarsoL3uyivY0U6DUwmZAqHurlioqDNUE2SyyH29LtEZCNGjYEuTDUCi6Bz99FZeJtnbZxcdghA9CjTUOqGiL6PVW3klBpob6PsZCZkCwZZGe7CjTTXfxKW38koJNtU7RYcgRI+CrS0Itu3PF52jn2mkCxljs0azEs55xeTEmZhgR1tWsKUxWnQOQvQo2LYf8PtH1QnTYcTyAlAGQAYw0qR+MgBVfAg61LZ/drB1v+gYhOhTwI9QX69q5vUasbw455rarZQD/uxge6voGIToltzbo5rDMmMuJ8bYDMbYIVMRZqJkb18c/D7RMQjRLbmv1yE6Q78D7TZ+gzGWCeB5AAuh7Co6GGNnAjiec37ZFOUbk5CnRzUPLCF6FPL2aXLLaw2ANwA4AfjD520EoJqvRZL7aIwXIVPK51XNc2ws5bUUwGrOeQjKlhc45x0A3FMRbDzkgM8iOgMheiYHQyaM/AbetBlLeTUC+M5UsIyxOQD2TWqiiZBlTb3BQIjmBAMSVDI+dCwh7gfwOmPsEgAmxth5ADYA+OOUJBsPKi9CppQcCEgYw7HyqTTqJzvnfB2AGwCcBaAawMUAfss5f26Kso2dLKtic5YQvZKDAQMAVcyXN6YG5Zy/BuC1KcoycZO95SVJgMEASIbw71L4dwMkgwEwABIMyjIGAwAJMIaXN0iQ8O1y6F/um3VKgCQpl0M5Twqfp1x/4DLStxnCPweeJ0kG5ShE+Ha/Xfbb9Unf3C7Cyw+4rfB1pW9uWwIkQJYMMiRJlqVv7pMMyRA+LcmQJMjK4yP3r082GGRIBgCSLH83t7J8+Dbk8PWJtgRnzXZCJeUlybI86oUZY5cCOA9AGoA6AC8AWMc5H/1KptCe2ta6suZuu4zwOwoDyMp/JHng6e9eBuDbyweeH4IMWQbk/p+ycpksywgpV5BDMiR5wHKhb5aTBywPhMKPd/hySYYshy+XQuFfQgNuJwRZUs4Lr0+WIQPSd26n/7RyGUIyJECGLMtS6Lt5pfCycgiyJIeUA6+yLEvfrF/5m5D673v//fzmMe3/fcBlGHT6m8d4wP0ffPrb9aviT4eM0rlLZrSctSgjF0CH6CxjGed1L4BTADwEoApAFoDrATAAN05JujHa0+atu3Nj2RLROQjRq84+fx+AgOgcwNh2G38GYDHnvKb/DMbY6wC2QCXlJUkIis5AiJ5FGQ0hqKS8xnKMqCv8b/B5qpkw3iBJIdEZCNEzk8Eg49tB6kKNZUqchwC8zBhbDaAGwAwo7z4+OHXxxsZAW16ETCmjQTmsKjoHML4pcX40aJnlAB6bzFDjZTQYqLwImUJGg0EVW12AzqbEsZgMHtEZCNEzq8kw+NCRMJoqpwOxm02qeWAJ0SNrlLFbdIZ+YxkqYQJwJYAjASRgwK4k5/yHkx9t7GxRhv1mowG+oCp2yQnRHZuKymssW14PArgCwEcAlgB4CUASgPenINe4WEzGPYkOs+gYhOiW3WxUzd7NWMrrdAAncM4fBhAI/zwV3z+AL0yyy1Kc4LDQkG1CpoA1ygBrlLFZdI5+YymvaCgfyAaAXsZYNOe8BMCiyY81PnazqTrNbVXNKwMhepJgt8BiMpSKztFvLCPsiwEcDOALAF8BuIMx1gmgdiqCjVNtutvWBcAlOgghepPgMMspLmuJ6Bz9xlJev8a3Hwu4FsDjABwAVk52qAnoiLebabgEIVMg1WXttEUZaw685PQ40Aj75SOc97/hn2o6Qi5bo9RzQJEQPUlz23oA1IvO0e9AW15PjWIdMgDVfIuujcqLkCmRoOzVtIvO0e9AI+yzpyvIZHFaTcLnGSJEj5zWqP34/lR5wuhqhD0AuG1RJW6bKiZ6JERX4u1m1ewyAjosr6zY6DfmpDi9onMQoifxdjPctqidonMMpLvyskYZv14yI7ZRdA5C9KQw1dWbFRv9lugcA+muvAD0JDktDaJDEKInizJiGs0mwzbROQbSY3khwW5R1b45IVqX6LA0AugVnWMgXZaX22baEW9X0/AzQrRNbQfrAZ2W18w4+5uFqS5VvUoQolXxdjNcVpOqDtYDOi0vs8mwbVFGDB20J2QSzElx9c6Ms6vqYD2g0/IC0JfooIP2hEyGRRnuJrUdrAf0W15IdJjrRGcgRA9SXdZ6qOxgPaDj8kpyWt/IT3bStwkRMgG2KCNS3bZdonMMRbfllRFje/HY/OTqAy9JCBnOYdlx3bPi7X8VnWMoui0vAF3Z8dEVokMQomWH5yTstZmNX4nOMRQ9lxdSXNZNKS6r6BiEaJJBAtLc1j1QyTdkD6br8spNdKw7Nj+J3nUkZBzmpbn9aW7bC6JzDEfX5QWgtjDVtVd0CEK0aAVL2pfisr4hOsdw9F5eSHVZtzgsY5mqnxACAJmx0eVQ4RCJfrovr7xEx5ojcxNodlVCxiAjxoYUl/Vd0TlGovvyMhkNu34wM452HQkZg2Pzk+pmJdifEZ1jJLovLwByuttWHGWUROcgRDPmKMeKVf354EgoL2TFRT98bH6yar71hBA1y0mwBzNjo18SneNAIqK83Laoz4+enVQsOgchWnDO4oyy7Hj7E6JzHEhElBcAzIi1vZCX6AgceElCIld0lBG5iY4voOJ3GftFTHnNjLevPWdxRpnoHISo2Snz05ryk52/F51jNCKmvAD05SY6NtOYL0KGd2h2XFGU0aCJF/lIKi8Uprp+f8bCdNXNxU2IGizKcHszYmyqnEFiKBFVXgCqlmbF7qJBE4R832kL0vfMiI3eIDrHaEVaeWFGTPRjh2THeUTnIERN4qKjMDMu+n0AmpnAM+LKK9Vtff3keWlcdA5C1OSsRRk1BSmue0TnGIuIKy8AocxY2+vpbqssOgghamAxGbBkRuwWqHxE/WCRWF6YneRcffmybBq0SgiACw/OrCtMc10jOsdYRWR5AfCwZOczsxMdftFBCBHJaTFh2az4D81Gg+amTI/U8kJOguOBlcuyd4vOQYhIlxwys3JemltzW11ABJcXgEB2vP2hgzNj6Z1HEpES7GYsyYx5Cxo71tUvkssLWXHRz1y0NHOH6ByEiHD5suw9c1Jct4jOMV4RXV4A5Ky46DuPzU+imVZJRJkRawsVprr+BUCzf/uRXl5Ic9v+e9qC9K+NBhp3TyLHFcuyi2cnOe8SnWMiIr68AGBWgv36MxemN4vOQch0yE92+nITHU8D6BOdZSKovADERZu3rmBJn1mj6OEg+rfysOzdOQmOh0TnmCh6toYVpDh/eeUROfRFHUTXTluQ1jIrwX4jNPQZxuFQeYVZTMaagzNjH1s8I0b1M0gSMh5JDot86vy01zNibKr+SrPRovIaIDfR8eAvls2i3UeiSzesmL1rTorrl6JzTBZ6ln6XvGhGzEWrjszV3EclCBnJafPTWvISHdcB0M2gbCqv76tblBHz8MGZsT2igxAyGRIdFvmU+WlvpMfYNorOMpmovIaQm+h49PJl2Z/ZooyioxAyYTeumL2rMNV1pegck43Ka2jywoyYi1YdlVMuOgghE3Gqsrt4PXS0u9iPymt4DYsyYh74wcy4btFBCBmPROXdxTfTY2zviM4yFai8RpCT4Hhi5WEzP7WbafeRaItBAm45hulyd7EfldfI5AXpMRfcfsKc3fTRR6Ilq47MrZqT6roQgG7feKLyOrD9hanOC1YdlVslOggho/GTuakth2bH3xpvN+t6uicqr1FIclq3Hzoz/pZT5qW2iM5CyEjmprr6zlmc8fSsBPt60VmmGpXXKM1KsD9/5qKMp+anu+njQ0SVEuxmXLs87x2W7LxJdJbpQOU1BvnJzluu/VHef5OdFvraNKIqZqMBd5w456t5ae7zAETE3yeV19jIhamu828/oeALi4keOqIetxzLShdmxM/bXuQAAAhISURBVJwOHY7nGg49A8eu7+CsuNN+e3w+fes2UYWLl2bWLUh3/4/FZKgWnWU6UXmNT/38tJifX7Esu0Z0EBLZjsiJ7zy2IPnBGbHR74nOMt2ovMYp1W399Ki8xNvPXJjeJDoLiUwHZ8b2XL5s1rrZSc77RWcRgcprAnITHetOX5j++1PmpdL892RaLUh3e646MufZ/GTntaKziELlNUF5iY4/n7N4xj0nFqbsF52FRIY5KU7vdcvzNhSkuK5EhLyzOBQqr0mQl+R48PyDZtx3bH5yq+gsRN9YksN70zHspYIU188RwcUFAJIsR/T9n1S8sevm576qvv6tooZ40VmI/hSmuvpuWjH7XwUprosBhETnEY3Ka5KVNnVd/eLW2ltf2VGXKDoL0Y8F6W7PdcvzXihIcV2GCN/i6kflNQXKmruveG1H3R0vbKlJEZ2FaN9BmbHdq47M+UekH+MajMprilS09Fy0saRx9ZOfVaaJzkK0awVLavvp0synC1Jc14OK6zuovKZQdZvn6B21HY/fs5HneQMRf4iCjIEE4PJl2TVH5SX+ITfRsUZ0HjWi8ppi3kAwY3ttx8t3vFl8UHO3l6Y0JAdkjTLgN8cVlMxLc12a5rZ9JjqPWlF5TQ/brvqO5x98v/TYHXWdNtFhiHqluKzy7ScUfH5QZuypABpF51EzKq/pI/HGrrtf2lZ7Gb0TSYayKCPGs+qo3LfmprouAOAVnUftqLym2d79PWd9UdV674Pvl80M0mNPws5YkN582oK0NSzZ+TvQgflRofISoM3jK9hV17nhzreK5nX0BUTHIQIZJQnXLs/buzQr9saZ8fZ/ic6jJVRe4ri31bT/8/FPKo7YUt1Ox8EiUFZcdOC65Xnb8pOdF8dGm4tE59EaKi+xpLLm7l9vqW5f9ehHZdl9fhpOEQkMEnDR0qz6FSxpQ36y80YAftGZtIjKSx3St1a3/33t5opDv9rXHi06DJk6mbG24PVHz96al+i4IsFh2SI6j5ZReamHVNbcfdW2mvZfP/Jh+axef1B0HjKJDBJw4cGZDStY0oaCFNcNoK2tCaPyUp/UrTXtf39y897Dvqhqs4sOQyYuM9YWvO7o2dtyEuy/SHZavxKdRy+ovNRJKmvu/p/ttR3XPLKpLNdDW2GaZJCACw7ObDiGJb1YkOK6DrS1NamovNQteVtN+5NvFTUc+tqO+ngaF6YdP8xN6D5ncca2mXH2a5Ocli9F59EjKi8NaO72/qByf8/ql7fXLXqXN7lF5yHDm5/m9l5ySNbOrLjoe2bERr8CGnA6Zai8NKS2vffE6jbP7579ct+8L6ra6F1JFcmOtwcuX5ZdlBNvfyI7wb4GNNPplKPy0h5pX6vnwsrWnquf3Fw5t7ixyyw6UCRLdlpw+bLskoJk1/q8JMcfAfhEZ4oUVF7aZSxv6b6qvLnn52s37y2oavUYRQeKJG6rCT8/NLtiYYb7PwUprt8A6BadKdJQeWmfZU9T122V+z0nv7qjjn25r80qOpCezU5yBM5ZnFGak+D4tDDVdSsA+s5OQai89MNQ19F7WnVb7y++qGqd+/L2upRuL33oezJEGSUcX5DStnx2YvGM2Oj1WXHRT4KmrBGOykufsnbXd/6moqV72Ytba/OKG7tMogNpUYrLinMXZ1QUprq25iTYVzutUTTAVEWovPTNXNXac0l1W+8FH5e3zHljd0M8zaU/MgnAodnxnpPnpZZkxka/mZfkeABAu+hc5PuovCJEjzcwv6yl++aa9t7Cz/a2zvykvMXV46OR+wBgNhqwNCvWc2ReYlVGjK00I8b21xSX9Q3QGC1Vo/KKPJI3EFxY3tKzsr6jb0FxY9esjSWNKXUdfaJzTSu3LQrL8xJbD8qMrUxxW4uyYqPXuW1RHwGgRtcIKi+SVtHS/bOGTu9Rla2eWe/xxqxd9Z2mkA7/LDJjbTiuILl2dpJzb4rL+mV+snMtgBLRucj4UHmRgaLrO/p+Ut/Ze1ZTlzezqcubtLWmPbmoodPa6tHWZ4odFhMKkp3+JZkxjakuW1O83VyX5LRsyo63PwugSXQ+MnFUXmQkdl8gtKCytefEzr5AYUu3N7Wlx5e8raY9eXd9p62lRx2Dye1mI/KTnYElmbENaW5rc3y0uS4m2lya5ra+6bJGfQGgQ3RGMvmovMhY2YIheV5FS8+JHX3+eW0eX2KvP+j0BkLOlm6vra6jz9HQ1efc3+2TWnp8mOikiiaDhHi7GYkOC1Jc1u50t7UryWnttUYZuqxRxi6nxdQRG22uSHVZ33Tboj4H0DY5d5OoHZUXmSwSgFgAqT2+QEZjp3e2NxDM9QVCiT2+oNPjDzoCoZBFlmEIybJBlmGQZUgAJElCSJIQMkhSUJIQMkpS0GIyeuxmY1e02dgWZTSUJzosxW5bVBWAOgD7QR98jnhUXoQQTTKIDkAIIeNB5UUI0SQqL0KIJlF5EUI0icqLEKJJNFUKUQXG2PEAHgZgBPAk53y14EhE5WioBBGOMWYEsAfAMQBqAHwJ4DzOeZHQYETVaLeRqMFSAGWc8wrOuQ/ACwBOEZyJqByVF1GDdADVA07XhM8jZFhUXoQQTaLyImpQC2DGgNMZ4fMIGRa920jU4EsAeYyxbCildS6A88VGImpHW15EOM55AMCvALwNoBjAPznnu8WmImpHQyUIIZpEW16EEE2i8iKEaBKVFyFEk6i8CCGaROVFCNEkKi9CiCZReRFCNInKixCiSVRehBBNovIihGgSlRchRJOovAghmkTlRQjRJCovQogmUXkRQjSJyosQoklUXoQQTaLyIoRoEpUXIUST/h/yELbwK8k3gAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Classes:  [0 1]\n",
            "\n",
            "Number of One Hot encoded class labels:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx2-vcsRsVo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 300\n",
        "maxlen = 300\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "embedding_type = 'glove_embed_tokenizer'\n",
        "\n",
        "seed = 21\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsjEvxrwsWeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1631c8b3-2742-40e3-acae-0e765eb0cac3"
      },
      "source": [
        "%time\n",
        "\n",
        "# Splitting data into train, test & validation sets\n",
        "x_train, x_val_test, y_train, y_val_test = train_test_split(\n",
        "    train, label_onehot, test_size=.4, stratify=label_encoded, random_state=seed)\n",
        "\n",
        "x_val, x_test, y_val, y_test = train_test_split(\n",
        "    x_val_test, y_val_test, test_size=.6, stratify=y_val_test, random_state=seed)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.72 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3orvnL4RssW2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "17fd7c81-e5e6-4957-ddae-1424ec84ed78"
      },
      "source": [
        "# Create the tokenizer & fit on all the url texts\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, lower=True)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "# Dumping tokenizer using joblib which is faster than pickle\n",
        "joblib.dump(tokenizer, os.path.join('/content/tokenizer.pickle'))\n",
        "\n",
        "# Generating sequences & padding for efficient training of our neural network\n",
        "# Transforms each text in texts to a sequence of integers.\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "train_padded_sequences = sequence.pad_sequences(train_sequences, maxlen=maxlen)\n",
        "\n",
        "val_sequences = tokenizer.texts_to_sequences(x_val)\n",
        "val_padded_sequences = sequence.pad_sequences(val_sequences, maxlen=maxlen)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "test_padded_sequences = sequence.pad_sequences(test_sequences, maxlen=maxlen)\n",
        "\n",
        "del x_train, train_sequences, x_val, val_sequences, x_test, test_sequences\n",
        "\n",
        "\n",
        "# Load Glove embeddings\n",
        "\n",
        "print('Loading Glove word vectors ...')\n",
        "embeddings_index = dict()\n",
        "\n",
        "try:\n",
        "    f = open('/content/glove.6B.300d.txt')\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "except Exception as e:\n",
        "    print('Stacktrace', e)\n",
        "    print('Glove file does not exist.')\n",
        "finally:\n",
        "    f.close()\n",
        "\n",
        "print('Loaded %s Glove word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "# Create a weight matrix for all the words\n",
        "\n",
        "print('\\nMapping words to Glove embeddings ...')\n",
        "embedding_matrix = np.zeros((vocab_size, maxlen))\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index > vocab_size - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "\n",
        "print('Shape of Embedding Matrix: ', embedding_matrix.shape)\n",
        "print('Glove Mapping done.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Glove word vectors ...\n",
            "Loaded 400000 Glove word vectors.\n",
            "\n",
            "Mapping words to Glove embeddings ...\n",
            "Shape of Embedding Matrix:  (300, 300)\n",
            "Glove Mapping done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCiTnNBjxEIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking labels, predictions shape\n",
        "\n",
        "\n",
        "def check_units(y_true, y_pred):\n",
        "\n",
        "    if (y_pred.shape[1] != 1):\n",
        "        y_pred = y_pred[:, 1:2]\n",
        "        y_true = y_true[:, 1:2]\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "\n",
        "# Calculate Precision\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "\n",
        "    y_true, y_pred = check_units(y_true, y_pred)\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "\n",
        "    return precision\n",
        "\n",
        "\n",
        "# Calculate Recall\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "\n",
        "    y_true, y_pred = check_units(y_true, y_pred)\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "    return recall\n",
        "\n",
        "\n",
        "# Calculate F1 Score\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "\n",
        "    # Calculate Recall\n",
        "\n",
        "    def recall(y_true, y_pred):\n",
        "\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "        return recall\n",
        "\n",
        "    # Calculate Precision\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "\n",
        "        return precision\n",
        "\n",
        "    y_true, y_pred = check_units(y_true, y_pred)\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFTj6DkgvoSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e42698c-ba30-4160-ebca-64d53aa25575"
      },
      "source": [
        "# Initializing & running keras model\n",
        "\n",
        "print('\\n\\n****************** Keras model training started. *******************\\n\\n')\n",
        "\n",
        "# Setting checkpoint & early stopping\n",
        "checkpoint_path = os.path.join('/content/checkpoint','model_best_weights.{epoch:02d}-{val_accuracy:.4f}.hdf5')\n",
        "checkpoint = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max')\n",
        "\n",
        "# prefroc_callback = PrefrocCallback(training_data=(train_padded_sequences, y_train), validation_data=(val_padded_sequences, y_val))\n",
        "\n",
        "tensorboard = TensorBoard(\n",
        "    log_dir='./logs', histogram_freq=2000, write_graph=True, write_images=False)\n",
        "\n",
        "callbacks_list = [checkpoint, tensorboard]\n",
        "\n",
        "# Running the pipeline\n",
        "# Fixing the seed again\n",
        "np.random.seed(seed)\n",
        "\n",
        "print('\\n\\nBuild model ...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300, input_length=maxlen,\n",
        "                    weights=[embedding_matrix], trainable=False))\n",
        "model.add(LSTM(maxlen, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy', f1, precision, recall])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "print('Training starts...')\n",
        "model.fit(train_padded_sequences, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(val_padded_sequences, y_val),\n",
        "          verbose=2, callbacks=callbacks_list)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "****************** Keras model training started. *******************\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Build model ...\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 300, 300)          90000     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 300)               721200    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 602       \n",
            "=================================================================\n",
            "Total params: 811,802\n",
            "Trainable params: 721,802\n",
            "Non-trainable params: 90,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training starts...\n",
            "Train on 12480 samples, validate on 3328 samples\n",
            "Epoch 1/10\n",
            " - 234s - loss: 0.3910 - accuracy: 0.8232 - f1: 0.8162 - precision: 0.8122 - recall: 0.8387 - val_loss: 0.3062 - val_accuracy: 0.8720 - val_f1: 0.8735 - val_precision: 0.8484 - val_recall: 0.9057\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.87200, saving model to /content/checkpoint/model_best_weights.01-0.8720.hdf5\n",
            "Epoch 2/10\n",
            " - 230s - loss: 0.2922 - accuracy: 0.8774 - f1: 0.8763 - precision: 0.8686 - recall: 0.8945 - val_loss: 0.2890 - val_accuracy: 0.8765 - val_f1: 0.8672 - val_precision: 0.9033 - val_recall: 0.8407\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.87200 to 0.87650, saving model to /content/checkpoint/model_best_weights.02-0.8765.hdf5\n",
            "Epoch 3/10\n",
            " - 229s - loss: 0.2564 - accuracy: 0.8895 - f1: 0.8871 - precision: 0.8820 - recall: 0.9009 - val_loss: 0.2603 - val_accuracy: 0.8900 - val_f1: 0.8884 - val_precision: 0.8751 - val_recall: 0.9078\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.87650 to 0.89002, saving model to /content/checkpoint/model_best_weights.03-0.8900.hdf5\n",
            "Epoch 4/10\n",
            " - 229s - loss: 0.2374 - accuracy: 0.9019 - f1: 0.9001 - precision: 0.8887 - recall: 0.9194 - val_loss: 0.2627 - val_accuracy: 0.8891 - val_f1: 0.8921 - val_precision: 0.8476 - val_recall: 0.9465\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.89002\n",
            "Epoch 5/10\n",
            " - 228s - loss: 0.2132 - accuracy: 0.9106 - f1: 0.9086 - precision: 0.9059 - recall: 0.9188 - val_loss: 0.2364 - val_accuracy: 0.9014 - val_f1: 0.9013 - val_precision: 0.8785 - val_recall: 0.9292\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.89002 to 0.90144, saving model to /content/checkpoint/model_best_weights.05-0.9014.hdf5\n",
            "Epoch 6/10\n",
            " - 228s - loss: 0.1891 - accuracy: 0.9224 - f1: 0.9212 - precision: 0.9142 - recall: 0.9336 - val_loss: 0.2320 - val_accuracy: 0.9084 - val_f1: 0.9084 - val_precision: 0.8842 - val_recall: 0.9380\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90144 to 0.90835, saving model to /content/checkpoint/model_best_weights.06-0.9084.hdf5\n",
            "Epoch 7/10\n",
            " - 228s - loss: 0.1667 - accuracy: 0.9309 - f1: 0.9289 - precision: 0.9252 - recall: 0.9378 - val_loss: 0.2374 - val_accuracy: 0.9072 - val_f1: 0.9023 - val_precision: 0.9238 - val_recall: 0.8867\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90835\n",
            "Epoch 8/10\n",
            " - 229s - loss: 0.1588 - accuracy: 0.9348 - f1: 0.9328 - precision: 0.9289 - recall: 0.9412 - val_loss: 0.2263 - val_accuracy: 0.9105 - val_f1: 0.9095 - val_precision: 0.8919 - val_recall: 0.9322\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90835 to 0.91046, saving model to /content/checkpoint/model_best_weights.08-0.9105.hdf5\n",
            "Epoch 9/10\n",
            " - 229s - loss: 0.1280 - accuracy: 0.9470 - f1: 0.9460 - precision: 0.9419 - recall: 0.9537 - val_loss: 0.2409 - val_accuracy: 0.9053 - val_f1: 0.9013 - val_precision: 0.9113 - val_recall: 0.8961\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.91046\n",
            "Epoch 10/10\n",
            " - 227s - loss: 0.1040 - accuracy: 0.9613 - f1: 0.9601 - precision: 0.9602 - recall: 0.9624 - val_loss: 0.2475 - val_accuracy: 0.9144 - val_f1: 0.9146 - val_precision: 0.8936 - val_recall: 0.9410\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.91046 to 0.91436, saving model to /content/checkpoint/model_best_weights.10-0.9144.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7ff6ae449748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilo7BLX__cb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6a904736-2656-4c41-a9d8-b86dd0bcd859"
      },
      "source": [
        "# Evaluate the model\n",
        "\n",
        "scores = model.evaluate(test_padded_sequences, y_test,\n",
        "                        batch_size=batch_size)\n",
        "print('Loss:', scores[0])\n",
        "print('Accuracy:', scores[1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4992/4992 [==============================] - 9s 2ms/step\n",
            "Loss: 0.24642074818555743\n",
            "Accuracy: 0.9114583134651184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-Upe--E_5zM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}